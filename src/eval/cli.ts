/**
 * A/B Testing CLI
 * 
 * A simplified CLI interface for running A/B prompt evaluations
 */

import fs from 'fs';
import path from 'path';
import { Command } from 'commander';
import chalk from 'chalk';
import { ABEvaluationOptions } from './models/ab-types';
import { runABEvaluation } from './runners/ab-runner';
import { testCases, getQuickTestCases } from './models/test-cases';
import { getToolFriendlyName } from './utils/tools';

/**
 * Setup the evaluation CLI
 */
export function setupEvalCLI(): Command {
  // Create a command line interface
  const program = new Command();
  
  program
    .name('eval')
    .description('qckfx Agent Evaluation CLI')
    .version('1.0.0');
  
  // Add the eval command for A/B testing
  program
    .command('run')
    .description('Run A/B testing evaluation')
    .option('-c, --config <path>', 'Path to a custom A/B test configuration file')
    .option('-o, --output <dir>', 'Directory to save results (default: ./evaluation-results)')
    .option('-r, --runs <number>', 'Number of runs per test case (default: 3)')
    .option('--concurrency <number>', 'Number of concurrent test executions (default: 2)')
    .option('--quick', 'Use a smaller subset of test cases for quicker evaluation')
    .option('--no-judge', 'Disable AI judge evaluation')
    .action(async (options) => {
      try {
        let config;
        
        // Load either the provided config or the default one
        if (options.config) {
          console.log(chalk.blue(`Loading custom configuration from ${options.config}`));
          
          if (!fs.existsSync(options.config)) {
            console.error(chalk.red(`Configuration file not found: ${options.config}`));
            process.exit(1);
          }
          
          config = JSON.parse(fs.readFileSync(options.config, 'utf8'));
        } else {
          // Use the default config
          const defaultConfigPath = path.join(__dirname, './examples/ab-config-example.json');
          console.log(chalk.blue(`Loading default configuration from ${defaultConfigPath}`));
          
          if (!fs.existsSync(defaultConfigPath)) {
            console.error(chalk.red(`Default configuration file not found. Please provide a custom config with --config.`));
            process.exit(1);
          }
          
          config = JSON.parse(fs.readFileSync(defaultConfigPath, 'utf8'));
        }
        
        // Validate the configuration
        if (!config.configA || !config.configB) {
          console.error(chalk.red('Invalid configuration: must include configA and configB configurations'));
          process.exit(1);
        }
        
        // Handle the case where test cases might be in the config or use default ones
        let evaluationTestCases = config.testCases;
        if (!evaluationTestCases || evaluationTestCases.length === 0) {
          // Use all test cases or quick mode if requested
          evaluationTestCases = options.quick ? getQuickTestCases() : testCases;
          console.log(chalk.yellow(`No test cases in config, using ${options.quick ? 'quick' : 'all'} built-in test cases`));
        }

        // Set default values
        const abOptions: ABEvaluationOptions = {
          configA: config.configA,
          configB: config.configB,
          testCases: evaluationTestCases,
          runsPerTest: options.runs ? parseInt(options.runs, 10) : 3,
          enableJudge: options.judge !== false,
          concurrency: options.concurrency ? parseInt(options.concurrency, 10) : 2,
          outputDir: options.output || path.join(process.cwd(), 'evaluation-results'),
        };
        
        console.log(chalk.blue(`Starting A/B evaluation with ${abOptions.testCases.length} test cases`));
        console.log(chalk.blue(`Comparing: ${config.configA.name} vs ${config.configB.name}`));
        console.log(chalk.blue(`Runs per test: ${abOptions.runsPerTest}`));
        console.log(chalk.blue(`Concurrency: ${abOptions.concurrency}`));
        console.log(chalk.blue(`Judge enabled: ${abOptions.enableJudge}`));
        
        // Run the evaluation
        const result = await runABEvaluation(abOptions);
        
        console.log(chalk.green(`A/B evaluation completed successfully!`));
        console.log(chalk.green(`Total runs: ${result.runs.length}`));
        console.log(chalk.green(`Report: ${path.join(result.outputDir, 'ab-report.md')}`));
        
        // Print tool usage summary if available
        if (result.toolUsageAnalysis) {
          console.log(chalk.blue.bold(`\nTool Usage Summary:`));
          
          const usageA = result.toolUsageAnalysis[config.configA.id];
          const usageB = result.toolUsageAnalysis[config.configB.id];
          
          if (usageA && usageB) {
            console.log(`${config.configA.name}: ${usageA.avgTotal.toFixed(1)} tools/run (${usageA.avgUniqueTools.toFixed(1)} unique)`);
            console.log(`${config.configB.name}: ${usageB.avgTotal.toFixed(1)} tools/run (${usageB.avgUniqueTools.toFixed(1)} unique)`);
            
            // Show top 3 tools by usage for each config
            const topToolsA = Object.entries(usageA.avgCounts)
              .sort(([, countA], [, countB]) => countB - countA)
              .slice(0, 3);
              
            const topToolsB = Object.entries(usageB.avgCounts)
              .sort(([, countA], [, countB]) => countB - countA)
              .slice(0, 3);
            
            if (topToolsA.length > 0) {
              console.log(`\n${config.configA.name} top tools: ${topToolsA.map(([tool, count]) => 
                `${getToolFriendlyName(tool)} (${count.toFixed(1)})`).join(', ')}`);
            }
            
            if (topToolsB.length > 0) {
              console.log(`${config.configB.name} top tools: ${topToolsB.map(([tool, count]) => 
                `${getToolFriendlyName(tool)} (${count.toFixed(1)})`).join(', ')}`);
            }
          }
        }
        
        // Display summary if judging was enabled
        if (abOptions.enableJudge && result.averageJudgment) {
          console.log(chalk.yellow('\nJudgment Summary:'));
          
          // Display average scores for both configurations
          const configAName = config.configA.name;
          const configBName = config.configB.name;
          
          console.log(chalk.cyan(`\n${configAName} average scores:`));
          for (const [dimension, score] of Object.entries(result.averageJudgment[config.configA.id])) {
            if (dimension === 'overall') continue;
            console.log(`  ${dimension}: ${(score as number).toFixed(2)}`);
          }
          
          console.log(chalk.cyan(`\n${configBName} average scores:`));
          for (const [dimension, score] of Object.entries(result.averageJudgment[config.configB.id])) {
            if (dimension === 'overall') continue;
            console.log(`  ${dimension}: ${(score as number).toFixed(2)}`);
          }
          
          // Display comparison result if available
          if (result.comparison) {
            console.log(chalk.green('\nComparison Result:'));
            console.log(`  Winner: ${result.comparison.winner === 'A' ? configAName : (result.comparison.winner === 'B' ? configBName : 'Tie')}`);
            
            if (result.comparison.overallImprovement) {
              const improvement = result.comparison.overallImprovement;
              console.log(`  Overall Improvement: ${improvement > 0 ? '+' : ''}${improvement.toFixed(1)}%`);
            }
          }
        }
      } catch (error) {
        console.error(chalk.red('A/B evaluation failed:'), error);
        process.exit(1);
      }
    });

  // Add a list command to show available test cases
  program
    .command('list')
    .description('List available test cases')
    .option('--quiet', 'Display only test IDs and names without details')
    .action((options) => {
      console.log(chalk.yellow('Available test cases:'));
      
      testCases.forEach((testCase, index) => {
        console.log(`${index + 1}. ${chalk.cyan(testCase.name)} (${chalk.gray(testCase.id)})`);
        
        if (!options.quiet) {
          console.log(`   Type: ${testCase.type || 'unspecified'}`);
          console.log(`   Instructions: ${testCase.instructions.slice(0, 100) + '...'}`);
          console.log();
        }
      });
    });

  return program;
}

// Export the setup function as the default export
export default setupEvalCLI;